{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://capitalwholesalediamonds.com/product-category/round/\"\n",
    "response = requests.get(base_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('web_scraped/round/images'):\n",
    "    os.makedirs('web_scraped/round/images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to add all the summary data to the csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_divs = soup.find_all('div', class_ = 'ftc-product product')\n",
    "\n",
    "for product_div in product_divs:\n",
    "    \n",
    "    product_url = (product_div.find('a')).get('href')\n",
    "    response = requests.get(product_url)\n",
    "    product_page = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "summary_div = product_page.find('div', class_ =\"panel entry-content wc-tab\")\n",
    "summary_txt = summary_div.find('p').get_text()\n",
    "\n",
    "# Split the text content into lines\n",
    "lines = summary_txt.split('\\n')\n",
    "\n",
    "# Create a dictionary to store the extracted data\n",
    "data = {}\n",
    "for line in lines:\n",
    "    if line:\n",
    "        key, value = line.split(':', 1)\n",
    "        data[key.strip()] = value.strip()\n",
    "\n",
    "csv_file_path = \"/home/levi/Desktop/Work/Diamond Project/web_scraped/round/data_round.csv\"\n",
    "\n",
    "# Write the data into a CSV file\n",
    "with open(csv_file_path, mode='a', newline='') as csv_file:\n",
    "    fieldnames = data.keys()\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    # writer.writeheader()\n",
    "    writer.writerow(data)\n",
    "\n",
    "print(f\"Data saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to store the image files with the appropriate name corresponding to their Stock Number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_divs = soup.find_all('div', class_ = 'ftc-product product')\n",
    "\n",
    "for product_div in product_divs:\n",
    "    \n",
    "    product_url = (product_div.find('a')).get('href')\n",
    "    response = requests.get(product_url)\n",
    "    product_page = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "img_url = product_page.find('div', class_ = \"iconic-woothumbs-images-wrap\").find('img').get('src')\n",
    "\n",
    "# Send a request to the image URL\n",
    "img_response = requests.get(img_url)\n",
    "\n",
    "# Get the image data\n",
    "img_data = img_response.content\n",
    "\n",
    "# to do it in one line we can employ it as img_data = requests.get(product_page.find('div', class_ = \"iconic-woothumbs-images-wrap\").find('img').get('src')).content\n",
    "\n",
    "# # Save the image to a local file\n",
    "with open((f\"web_scraped/round/images/{data['Stock Number']}.jpg\") , \"wb\") as img_file:\n",
    "    img_file.write(img_data)\n",
    "\n",
    "print(\"Image download and saving completed.\")\n",
    "\n",
    "summary_div = product_page.find('div', class_ =\"panel entry-content wc-tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing both of them together to iterate through the whole page. We convert this in to function call. Make sure to initialize the csv headers according to the summary data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"/home/levi/Desktop/Work/Diamond Project/web_scraped/round/data_round.csv\"\n",
    "\n",
    "\n",
    "def page_iterator(base_url):\n",
    "\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    product_divs = soup.find_all('div', class_ = 'ftc-product product')\n",
    "\n",
    "    for product_div in product_divs:\n",
    "        \n",
    "        product_url = (product_div.find('a')).get('href')\n",
    "        response = requests.get(product_url)\n",
    "        product_page = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            \n",
    "            summary_div = product_page.find('div', class_ =\"panel entry-content wc-tab\")\n",
    "            summary_txt = summary_div.find('p').get_text()\n",
    "            img_url = product_page.find('div', class_ = \"iconic-woothumbs-images-wrap\").find('img').get('src')\n",
    "\n",
    "            # Send a request to the image URL\n",
    "            img_response = requests.get(img_url)\n",
    "\n",
    "            # Get the image data\n",
    "            # to do it in one line we can employ it as img_data = requests.get(product_page.find('div', class_ = \"iconic-woothumbs-images-wrap\").find('img').get('src')).content\n",
    "            img_data = img_response.content\n",
    "\n",
    "           \n",
    "            # Split the text content into lines\n",
    "            lines = summary_txt.split('\\n')\n",
    "\n",
    "            # Create a dictionary to store the extracted data\n",
    "            data = {}\n",
    "            for line in lines:\n",
    "                if line:\n",
    "                    key, value = line.split(':', 1)\n",
    "                    data[key.strip()] = value.strip()\n",
    "\n",
    "            csv_file_path = \"/home/levi/Desktop/Work/Diamond Project/web_scraped/round/data_round.csv\"\n",
    "\n",
    "            # Write the data into a CSV file\n",
    "            with open(csv_file_path, mode='a', newline='') as csv_file:\n",
    "                fieldnames = data.keys()\n",
    "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "                # writer.writeheader()\n",
    "                writer.writerow(data)\n",
    "\n",
    "            print(f\"Data saved to {csv_file_path}\")\n",
    "\n",
    "            # Save the image to a local file\n",
    "            with open((f\"web_scraped/round/images/{data['Stock Number']}.jpg\") , \"wb\") as img_file:\n",
    "                img_file.write(img_data)\n",
    "\n",
    "            print(\"Image download and saving completed.\")\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"No image was found, hence no csv row was made, nor was a image saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://capitalwholesalediamonds.com/product-category/round/\"\n",
    "for i in range(1, 1411):\n",
    "    if i == 1:\n",
    "        base_url = \"https://capitalwholesalediamonds.com/product-category/round/\"\n",
    "        page_iterator(base_url)\n",
    "        print(f\"Web scrapping page number:{i} - Completed\")\n",
    "    else:\n",
    "        base_url = \"https://capitalwholesalediamonds.com/product-category/round/\"\n",
    "        base_url = os.path.join(base_url, f'page/{i}/')\n",
    "        page_iterator(base_url)\n",
    "        print(f\"Web scrapping page number:{i} - Completed\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
